{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d13c3f4-158e-44d7-8e4f-326d71143f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import get_dataset_config_names\n",
    "domains = get_dataset_config_names(\"subjqa\")\n",
    "domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1978a6fb-d5bc-4163-944d-8c84d213c54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are only interested in the electronics section\n",
    "from datasets import load_dataset\n",
    "subjqa = load_dataset(\"subjqa\", name = \"electronics\")\n",
    "print(subjqa[\"train\"][\"answers\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8970db1-80e9-4327-b035-606d9a820dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dfs = {split: dset.to_pandas() for split, dset in subjqa.flatten().items()}\n",
    "for split, df in dfs.items():\n",
    "    print(f\"Number of questions in {split}: {df['id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03461222-045f-4e35-a6a8-9098fc0d8098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on the columns you are interested in\n",
    "qa_cols = [\"title\", \"question\", \"answers.text\", \"answers.answer_start\", \"context\"]\n",
    "sample_df = dfs[\"train\"][qa_cols].sample(2, random_state = 7)\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5466f3b-b96a-4fc1-824f-2c18f8859d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will directly use a fine tuned model first. We choose MiniLM cause its small and easy to iterate on\n",
    "# Lets inspect the tokenizer first\n",
    "from transformers import AutoTokenizer\n",
    "model_ckpt = \"deepset/minilm-uncased-squad2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "question = \"How much music can this hold?\"\n",
    "context = \"\"\"an MP3 is about 1 MB/minute, so about 6000 hours depending on \\\n",
    "the file size.\"\"\"\n",
    "inputs = tokenizer(question, context, return_tensors = \"pt\")\n",
    "\n",
    "# Lets decode to see how it goes\n",
    "print(tokenizer.decode(inputs[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5f8789-dda3-431e-8bf1-5c923d11d2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "print(outputs)\n",
    "\n",
    "# To convert the outputs into an answer span, we first need to get the logits for the start and end token\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "\n",
    "print(f\"Input IDs shape: {inputs.input_ids.size()}\")\n",
    "print(f\"Start logits shape: {start_logits.size()}\")\n",
    "print(f\"End logits shape: {end_logits.size()}\")\n",
    "\n",
    "# Now to get the answer, we can compute the argmax over start and end tokens logits and then slice the span from the input\n",
    "import torch\n",
    "\n",
    "start_idx = torch.argmax(start_logits)\n",
    "end_idx = torch.argmax(end_logits) + 1\n",
    "answer_span = inputs[\"input_ids\"][0][start_idx:end_idx]\n",
    "answer = tokenizer.decode(answer_span)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0546ab27-89c5-495b-99eb-fc8932504776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Well this was a good disecting experience. In transformers, all these are wrapped nicely\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(\"question-answering\", model = model, tokenizer = tokenizer)\n",
    "pipe(question = question, context = context, topk = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0c3b66-dff7-4ce1-a133-381f8c0535fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you pass an impossible question, the pipeline maps it to an empty string (but its not really doing that)\n",
    "pipe(question = \"Why is there no data?\", context = context, handle_impossible_answers = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717a0bd6-c6ee-4116-bf70-d77562e362f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay enough tinkering. Now time to use haystack. Haystack needs two things:\n",
    "# 1. A document store to store documents and metadata\n",
    "# 2. Pipeline that combines all the components of a QA system to enable custom query flows, merging documents from multiple retrievers, and more\n",
    "\n",
    "# We are running eleastic search in a docker container (https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html)\n",
    "# import the store\n",
    "from haystack.document_stores import ElasticsearchDocumentStore\n",
    "document_store = ElasticsearchDocumentStore(host = \"localhost\", port = 9200, username=\"elastic\", \n",
    "    password=\"cMYVjbUMj_8_664gC6R8\", index=\"document\", scheme = \"https\", verify_certs = True,\n",
    "    ca_certs = \"/home/tanvir/work/qa-experiment/http_ca.crt\", return_embedding = True)\n",
    "\n",
    "#hide\n",
    "# It's a good idea to flush Elasticsearch with each notebook restart\n",
    "if len(document_store.get_all_documents()) or len(document_store.get_all_labels()) > 0:\n",
    "    document_store.delete_documents(\"document\")\n",
    "    document_store.delete_documents(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9c8f16-5a75-4d84-a0f6-8c34bcc30db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fields in meta can be used to apply filters during retrieval. We inclue the item_id and q_review_id columns\n",
    "# of subjQA so we can filter by product and question ID, along with the corresponding training split.\n",
    "# Now lets loop through and write the items\n",
    "for split, df in dfs.items():\n",
    "    # Exclude duplicate reviews\n",
    "    docs = [{\"content\": row[\"context\"], \n",
    "             \"meta\":{\"item_id\": row[\"title\"], \"question_id\": row[\"id\"], \n",
    "                     \"split\": split}} \n",
    "        for _,row in df.drop_duplicates(subset=\"context\").iterrows()]\n",
    "    document_store.write_documents(docs, index=\"document\")\n",
    "\n",
    "print(f\"Loaded {document_store.get_document_count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc70808d-5b1e-43b0-ab71-ca42c978bb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use Elasticsearch's default BM25 algorithm\n",
    "from haystack.nodes import BM25Retriever\n",
    "es_retriever = BM25Retriever(document_store=document_store)\n",
    "\n",
    "item_id = \"B0074BW614\"\n",
    "query = \"Is it good for reading?\"\n",
    "retrieved_docs = es_retriever.retrieve(query = query, top_k = 3, filters = {\"item_id\":[item_id], \"split\":[\"train\"]})\n",
    "\n",
    "print(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50d9c34-bc3d-4dfc-9072-16185fffc6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets initialize the reader\n",
    "#hide_output\n",
    "import os\n",
    "from haystack.reader.farm import FARMReader\n",
    "\n",
    "# Just a temporary workaround\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "model_ckpt = \"deepset/minilm-uncased-squad2\"\n",
    "max_seq_length, doc_stride = 384, 128\n",
    "\n",
    "reader = FARMReader(model_name_or_path = model_ckpt, progress_bar = False,\n",
    "                    max_seq_len = max_seq_length, doc_stride = doc_stride, \n",
    "                    return_no_answer = True, use_gpu = True)\n",
    "\n",
    "print(reader.predict_on_texts(question = question, texts = [context], top_k = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f90507-c710-44d4-b028-b8c870430ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets put everything together with a haystack pipeline\n",
    "from haystack.pipelines import ExtractiveQAPipeline\n",
    "pipeline = ExtractiveQAPipeline(reader, es_retriever)\n",
    "\n",
    "n_answers = 3\n",
    "preds = pipeline.run(query = query, params={\"Retriever\": {\"top_k\": 3}, \"Reader\": {\"top_k\": n_answers}, \"filters\": {\"item_id\": [item_id], \"split\":[\"train\"]}})\n",
    "\n",
    "print(f\"Question: {preds['query']} \\n\")\n",
    "\n",
    "for idx in range(n_answers):\n",
    "    print(f\"Answer {idx+1}: {preds['answers'][idx].answer}\")\n",
    "    print(f\"Review snippet: ...{preds['answers'][idx].context}...\")\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b9a4fbda-acae-49e3-97e4-e0a80a827405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1268 question-answer pairs\n"
     ]
    }
   ],
   "source": [
    "from haystack import Label, Answer, Document\n",
    "\n",
    "labels = []\n",
    "for i, row in dfs[\"test\"].iterrows():\n",
    "    # Metadata used for filtering in the Retriever\n",
    "    meta = {\"item_id\": row[\"title\"], \"question_id\": row[\"id\"]}\n",
    "    # Populate labels for questions with answers\n",
    "    if len(row[\"answers.text\"]):\n",
    "        for answer in row[\"answers.text\"]:\n",
    "            label = Label(\n",
    "                query = row[\"question\"],\n",
    "                answer = Answer(answer = answer),\n",
    "                document = Document(\n",
    "                    id = i, content_type = \"text\", content = row[\"context\"]\n",
    "                ),\n",
    "                is_correct_answer = True,\n",
    "                is_correct_document = True,\n",
    "                no_answer = False,\n",
    "                origin = \"user-feedback\",\n",
    "                meta = meta\n",
    "            )\n",
    "            labels.append(label)\n",
    "    # Populate labels for questions without answers\n",
    "    else:\n",
    "        label = Label(\n",
    "            query = row[\"question\"],\n",
    "            answer = Answer(answer = \"\"),\n",
    "            document = Document(\n",
    "                id = i, content_type = \"text\", content = row[\"context\"]\n",
    "            ),\n",
    "            is_correct_answer = True,\n",
    "            is_correct_document = True,\n",
    "            origin = \"user-feedback\",\n",
    "            meta = meta,\n",
    "            no_answer = True,\n",
    "        )\n",
    "        labels.append(label)\n",
    "        \n",
    "document_store.write_labels(labels, index = \"label\")\n",
    "print(f\"\"\"Loaded {document_store.get_label_count(index=\"label\")} \\\n",
    "question-answer pairs\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196cba6e-0982-4087-93d9-c9ce9dc8fcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_agg = document_store.get_all_labels_aggregated(\n",
    "    index = \"label\",\n",
    "    open_domain = True,\n",
    "    aggregate_by_meta = [\"item_id\"]\n",
    ")\n",
    "print(len(labels_agg))\n",
    "print(labels_agg[109])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c9bb0a-5048-4159-85b9-9f19c4e8c82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_pipeline(pipeline, top_k_retriever = 10, top_k_reader = 4):\n",
    "    eval_result = pipeline.eval(labels = labels_agg, params = {\"Retriever\": {\"top_k\": top_k_retriever}, \"Reader\": {\"top_k\": top_k_reader}})\n",
    "    metric = eval_result.calculate_metrics()\n",
    "    return metric['Retriever']['recall_single_hit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d6eef2-c146-43ea-bf43-9ddd00d54207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-53:\n",
      "Process ForkPoolWorker-51:\n",
      "Process ForkPoolWorker-52:\n",
      "Process ForkPoolWorker-50:\n",
      "Process ForkPoolWorker-49:\n",
      "Process ForkPoolWorker-48:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tanvir/.pyenv/versions/3.8.6/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tanvir/.pyenv/versions/3.8.6/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/tanvir/.pyenv/versions/3.8.6/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/tanvir/.pyenv/versions/3.8.6/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/tanvir/.pyenv/versions/3.8.6/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/tanvir/.pyenv/versions/3.8.6/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/tanvir/.pyenv/versions/3.8.6/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/tanvir/.pyenv/versions/3.8.6/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/tanvir/.pyenv/versions/3.8.6/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/tanvir/.pyenv/versions/3.8.6/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/tanvir/.pyenv/versions/3.8.6/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/tanvir/.pyenv/versions/3.8.6/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/tanvir/.pyenv/versions/3.8.6/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/tanvir/.pyenv/versions/3.8.6/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/tanvir/.pyenv/versions/3.8.6/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/tanvir/.pyenv/versions/3.8.6/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/tanvir/.pyenv/versions/3.8.6/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/tanvir/.pyenv/versions/3.8.6/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/tanvir/.pyenv/versions/3.8.6/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/tanvir/.pyenv/versions/3.8.6/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/tanvir/.pyenv/versions/3.8.6/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/tanvir/.pyenv/versions/3.8.6/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/tanvir/.pyenv/versions/3.8.6/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/tanvir/.pyenv/versions/3.8.6/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/tanvir/.pyenv/versions/3.8.6/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/tanvir/.pyenv/versions/3.8.6/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/tanvir/.pyenv/versions/3.8.6/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/tanvir/.pyenv/versions/3.8.6/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/tanvir/.pyenv/versions/3.8.6/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/tanvir/.pyenv/versions/3.8.6/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "#hide_output\n",
    "def evaluate_retriever(pipeline, topk_values = [1,3,5,10,20,40]):\n",
    "    topk_results = {}\n",
    "    for topk in topk_values:\n",
    "        recall_value = eval_pipeline(pipeline, top_k_retriever = topk)\n",
    "        topk_results[topk] = {\"recall\": recall_value}\n",
    "        \n",
    "    return pd.DataFrame.from_dict(topk_results, orient = \"index\")\n",
    "\n",
    "es_topk_df = evaluate_retriever(pipeline)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3768d65-2c9b-477c-bbf4-55a363cfc0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how the result improves as k increases\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_retriever_eval(dfs, retriever_names):\n",
    "    fig, ax = plt.subplots()\n",
    "    for df, retriever_name in zip(dfs, retriever_names):\n",
    "        df.plot(y = \"recall\", ax=ax, label=retriever_name)\n",
    "    plt.xticks(df.index)\n",
    "    plt.ylabel(\"Top-k Recall\")\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.show()\n",
    "    \n",
    "plot_retriever_eval([es_topk_df], [\"BM25\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0f3e75-d199-4f58-8bc8-b27d63d60fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see, the recall improves as we improve k. But can we do better? Lets use DPR\n",
    "from haystack.nodes import DensePassageRetriever\n",
    "\n",
    "dpr_retriever = DensePassageRetriever(\n",
    "    document_store = document_store,\n",
    "    query_embedding_model = \"facebook/dpr-question_encoder-single-nq-base\",\n",
    "    passage_embedding_model = \"facebook/dpr-ctx_encoder-single-nq-base\",\n",
    "    embed_title = False)\n",
    "\n",
    "# Important:\n",
    "# Now that after we have the DPR initialized, we need to call update_embeddings() to iterate over all\n",
    "# previously indexed documents and update their embedding representation.\n",
    "# While this can be a time consuming operation (depending on corpus size), it only needs to be done once.\n",
    "# At query time, we only need to embed the query and compare it the existing doc embeddings which is very fast.\n",
    "document_store.update_embeddings(dpr_retriever)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491d1cbb-cbb7-4e45-a9d8-6a0d16b47ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.utils import print_answers\n",
    "# We can use the same reader as before since we are only evaluating retriever\n",
    "dpr_pipe = ExtractiveQAPipeline(reader, dpr_retriever)\n",
    "\n",
    "# Try a question\n",
    "prediction = dpr_pipe.run(query = \"How does the fan work?\", params = {\"Retriever\": {\"top_k\": 10}, \"Reader\": {\"top_k\": 5}})\n",
    "print_answers(prediction, details = \"minimum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e5948e-84c1-408a-ac94-4b42df7ced65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_output\n",
    "# Now do a similar evaluation but with DRP retriever\n",
    "dpr_topk_df = evaluate_retriever(dpr_pipe)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac5d9cb-0447-42d5-8dfb-0df1d9c9c76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_retriever_eval([es_topk_df, dpr_topk_df], [\"BM25\", \"DPR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43af3f2-de7e-493a-af75-60659fb7fd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets take a little detour and try FAISS.\n",
    "# FAISS is a library for efficient similarity search on a cluster of dense vectors. The FAISSDocumentStore uses a SQL(SQLite in-memory be default) database\n",
    "# under-the-hood to store the document text and other meta data. The vector embeddings of the text are indexed on a FAISS Index that later is queried for\n",
    "# searching answers. The default flavour of FAISSDocumentStore is \"Flat\" but can also be set to \"HNSW\" for faster search at the expense of some accuracy.\n",
    "# Just set the faiss_index_factor_str argument in the constructor. For more info on which suits your use case: \n",
    "# https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index\n",
    "\n",
    "from haystack.document_stores import FAISSDocumentStore\n",
    "from haystack.utils import clean_wiki_text, convert_files_to_docs, fetch_archive_from_http, print_answers\n",
    "\n",
    "document_store_faiss = FAISSDocumentStore(faiss_index_factory_str = \"HNSW\")\n",
    "\n",
    "\n",
    "# Let's first get some GOT files that we want to use\n",
    "doc_dir = \"/mnt/sda/got_data\"\n",
    "s3_url = \"https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/wiki_gameofthrones_txt6.zip\"\n",
    "fetch_archive_from_http(url = s3_url, output_dir = doc_dir)\n",
    "\n",
    "# Convert files to dicts\n",
    "docs = convert_files_to_docs(dir_path = doc_dir, clean_func = clean_wiki_text, split_paragraphs = True)\n",
    "\n",
    "# Now, let's write the dicts containing documents to our DB.\n",
    "document_store_faiss.write_documents(docs)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8268b61f-4d02-4c59-847a-e241505b1e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import DensePassageRetriever\n",
    "from haystack.pipelines import ExtractiveQAPipeline\n",
    "\n",
    "faiss_retriever = DensePassageRetriever(\n",
    "    document_store = document_store_faiss,\n",
    "    query_embedding_model = \"facebook/dpr-question_encoder-single-nq-base\",\n",
    "    passage_embedding_model = \"facebook/dpr-ctx_encoder-single-nq-base\",\n",
    "    max_seq_len_query = 64,\n",
    "    max_seq_len_passage = 256,\n",
    "    batch_size = 16,\n",
    "    use_gpu = True,\n",
    "    embed_title = True,\n",
    "    use_fast_tokenizers = True,\n",
    ")\n",
    "# Important:\n",
    "# Now that after we have the DPR initialized, we need to call update_embeddings() to iterate over all\n",
    "# previously indexed documents and update their embedding representation.\n",
    "# While this can be a time consuming operation (depending on corpus size), it only needs to be done once.\n",
    "# At query time, we only need to embed the query and compare it the existing doc embeddings which is very fast.\n",
    "document_store_faiss.update_embeddings(faiss_retriever)\n",
    "\n",
    "faiss_reader = FARMReader(model_name_or_path = \"deepset/roberta-base-squad2\", use_gpu = True)\n",
    "faiss_pipe = ExtractiveQAPipeline(faiss_reader, faiss_retriever)\n",
    "\n",
    "faiss_prediction = faiss_pipe.run(query = \"Who created the Dothraki vocabulary?\", params = {\"Retriever\": {\"top_k\": 10}, \"Reader\": {\"top_k\": 5}})\n",
    "print_answers(faiss_prediction, details = \"minimum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce12ef96-3e9a-442e-a63c-ff19a8299185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section is not running yet\n",
    "from haystack.modeling.evaluation.squad import compute_f1, compute_exact\n",
    "from haystack.nodes.evaluator import EvalAnswers\n",
    "from haystack.pipeline import Pipeline\n",
    "\n",
    "def evaluate_reader(reader):\n",
    "    score_keys = ['top_1_em', 'top_1_f1']\n",
    "    eval_reader = EvalAnswers(skip_incorrect_retrieval = False)\n",
    "    eval_pipe = Pipeline()\n",
    "    eval_pipe.add_node(component = reader, name = \"QAReader\", inputs = [\"Query\"])\n",
    "    eval_pipe.add_node(component = eval_reader, name = \"EvalReader\", inputs = [\"QAReader\"])\n",
    "\n",
    "    for l in labels_agg:\n",
    "        doc = document_store.query(l.query, filters = {\"question_id\":[l.labels[0].meta['question_id']]})\n",
    "        _ = eval_pipe.run(query = l.query, documents = doc, labels = l)\n",
    "                \n",
    "    return {k:v for k,v in eval_reader.__dict__.items() if k in score_keys}\n",
    "\n",
    "print(\"Start eval\")\n",
    "reader_eval = {}\n",
    "reader_eval[\"Fine-tune on SQuAD\"] = evaluate_reader(reader)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa31926c-6bb8-490e-b4bb-a1bb69b25826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reader_eval(reader_eval):\n",
    "    fig, ax = plt.subplots()\n",
    "    df = pd.DataFrame.from_dict(reader_eval)\n",
    "    df.plot(kind=\"bar\", ylabel=\"Score\", rot=0, ax=ax)\n",
    "    ax.set_xticklabels([\"EM\", \"F1\"])\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "plot_reader_eval(reader_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4ef58842-2df5-4ed1-9565-ada39447ab89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now its time to train the model on SubjQA dataset. Squad model is pretty complicated so teh first step is to\n",
    "# create a paragraph array associated with each product id\n",
    "def create_paragraphs(df):\n",
    "    paragraphs = []\n",
    "    id2context = dict(zip(df[\"review_id\"], df[\"context\"]))\n",
    "    for review_id, review in id2context.items():\n",
    "        qas = []\n",
    "        # Filter for all question-answer pairs about a specific context\n",
    "        review_df = df.query(f\"review_id == '{review_id}'\")\n",
    "        id2question = dict(zip(review_df[\"id\"], review_df[\"question\"]))\n",
    "        # Build up the qas array\n",
    "        for qid, question in id2question.items():\n",
    "            # Filter for a single question ID\n",
    "            question_df = df.query(f\"id == '{qid}'\").to_dict(orient=\"list\")\n",
    "            ans_start_idxs = question_df[\"answers.answer_start\"][0].tolist()\n",
    "            ans_text = question_df[\"answers.text\"][0].tolist()\n",
    "            # Fill answerable questions\n",
    "            if len(ans_start_idxs):\n",
    "                answers = [\n",
    "                    {\"text\": text, \"answer_start\": answer_start}\n",
    "                    for text, answer_start in zip(ans_text, ans_start_idxs)]\n",
    "                is_impossible = False\n",
    "            else:\n",
    "                answers = []\n",
    "                is_impossible = True\n",
    "            # Add question-answer pairs to qas\n",
    "            qas.append({\"question\": question, \"id\": qid, \n",
    "                        \"is_impossible\": is_impossible, \"answers\": answers})\n",
    "        # Add context and question-answer pairs to paragraphs\n",
    "        paragraphs.append({\"qas\": qas, \"context\": review})\n",
    "    return paragraphs\n",
    "\n",
    "# Now we just nee to apply the function to each product id and store the results in file\n",
    "\n",
    "import json\n",
    "\n",
    "def convert_to_squad(dfs):\n",
    "    for split, df in dfs.items():\n",
    "        subjqa_data = {}\n",
    "        # Create `paragraphs` for each product ID\n",
    "        groups = (df.groupby(\"title\").apply(create_paragraphs)\n",
    "            .to_frame(name=\"paragraphs\").reset_index())\n",
    "        subjqa_data[\"data\"] = groups.to_dict(orient=\"records\")\n",
    "        # Save the result to disk\n",
    "        with open(f\"electronics-{split}.json\", \"w+\", encoding=\"utf-8\") as f:\n",
    "            json.dump(subjqa_data, f)\n",
    "            \n",
    "convert_to_squad(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "09264dc9-054f-4800-b445-fb8cf75f4a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.utils -  Using devices: CUDA\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 1\n",
      "INFO - haystack.modeling.data_handler.data_silo -  \n",
      "Loading data into the data silo ... \n",
      "              ______\n",
      "               |o  |   !\n",
      "   __          |:`_|---'-.\n",
      "  |__|______.-/ _ \\-----.|       \n",
      " (o)(o)------'\\ _ /     ( )      \n",
      " \n",
      "INFO - haystack.modeling.data_handler.data_silo -  LOADING TRAIN DATA\n",
      "INFO - haystack.modeling.data_handler.data_silo -  ==================\n",
      "INFO - haystack.modeling.data_handler.data_silo -  Loading train set from: electronics-train.json \n",
      "INFO - haystack.modeling.data_handler.data_silo -  Got ya 47 parallel workers to convert 1265 dictionaries to pytorch datasets (chunksize = 6)...\n",
      "INFO - haystack.modeling.data_handler.data_silo -   0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0  \n",
      "INFO - haystack.modeling.data_handler.data_silo -  /w\\   /w\\   /w\\   /w\\   /w\\   /w\\   /w\\   /|\\  /w\\   /w\\   /w\\   /w\\   /w\\   /w\\   /|\\  /w\\   /|\\  /|\\  /|\\  /|\\  /w\\   /w\\   /|\\  /|\\  /w\\   /w\\   /w\\   /w\\   /w\\   /w\\   /|\\  /w\\   /|\\  /|\\  /w\\   /|\\  /w\\   /w\\   /w\\   /w\\   /w\\   /w\\   /w\\   /w\\   /w\\   /w\\   /|\\\n",
      "INFO - haystack.modeling.data_handler.data_silo -  /'\\   / \\   /'\\   /'\\   / \\   / \\   /'\\   /'\\   /'\\   /'\\   /'\\   /'\\   / \\   /'\\   /'\\   / \\   /'\\   /'\\   /'\\   /'\\   / \\   / \\   /'\\   /'\\   / \\   / \\   / \\   /'\\   /'\\   /'\\   /'\\   /'\\   /'\\   /'\\   /'\\   /'\\   /'\\   /'\\   / \\   /'\\   / \\   /'\\   /'\\   / \\   / \\   /'\\   /'\\ \n",
      "Preprocessing Dataset electronics-train.json:   1%|█▋                                                                                                                      | 18/1265 [00:00<00:12, 96.62 Dicts/s]WARNING - haystack.modeling.data_handler.processor -  Answer 'These earbuds are nothing special. Their best attribute is that they are cheap. The sound from them is nothing special. In comparison to my standard white IPod buds these seem to emphasize the higher frequencies. There seems to be nothing happening in the bass range with these. I don't think they are efficient enough to have much bass at all. To be fair, my IPod buds overly emphasize bass so they are not that accurate either. I also compared these to a pricy Sennheiser set that I've owned for a couple of years and let's just say there is a reason why the Sennheisers cost more.Surely these Panasonic buds are very nice to use if you think they might get damaged somehow in use as it will not break your heart to have to replace them. These might be especially useful if you only listen to talk radio or have high frequency hearing loss. If you really like to listen to the most of your music and you must wear earbuds then these are not for you.I also have to note that these are packaged in a difficult to open blister pack that required several passes with utility shears to break into. I hate to see all the plastic that has to be discarded just to get this product out of the package.Cosmetically, they are OK. They don't hurt my ears and are comfortable to me. They come with a couple of other size ear pieces if you are hard to fit. The wires take a set when folded so they are not very well-behaved and don't coil nicely and lie flat when you put them away. These will be a little bird's nest of tangled wires when you set them aside. ANSWERNOTFOUND' not contained in context.\n",
      "Example will not be converted for training/evaluation.\n",
      "Preprocessing Dataset electronics-train.json:   4%|████▌                                                                                                                  | 48/1265 [00:00<00:10, 117.29 Dicts/s]WARNING - haystack.modeling.data_handler.processor -  Answer using start/end indices is '  Operation of the menus and contro' while gold label text is 'Operation of the menus and controls'.\n",
      "Example will not be converted for training/evaluation.\n",
      "WARNING - haystack.modeling.data_handler.processor -  Answer using start/end indices is '  This camera performs like the pros.  Fast accurate and easy to operat' while gold label text is 'This camera performs like the pros.  Fast accurate and easy to operated'.\n",
      "Example will not be converted for training/evaluation.\n",
      "Preprocessing Dataset electronics-train.json:  32%|█████████████████████████████████████▍                                                                                | 402/1265 [00:01<00:01, 477.26 Dicts/s]WARNING - haystack.modeling.data_handler.processor -  Answer 'enjoy my music.  When I used my old one ear headset for music, the connection was not that good even if I put the phone in my pants pocket.  I used it on iPhone 4S and iPad 3 for music, phone calls, and audible app.  All Bluetooth controls work fine. ANSWERNOTFOUND' not contained in context.\n",
      "Example will not be converted for training/evaluation.\n",
      "Preprocessing Dataset electronics-train.json:  37%|████████████████████████████████████████████▏                                                                         | 474/1265 [00:01<00:01, 473.37 Dicts/s]WARNING - haystack.modeling.data_handler.processor -  Answer 'is good.  First unit defective.  Directions are weak and limited, Net support just plain bad. Roku insists on registration, [I think SEVEN times for us] before asking dumb questions and forcing us to run from computer to TV and back.  This is a Roku 3, and apparently it's too new for them to handle. ANSWERNOTFOUND' not contained in context.\n",
      "Example will not be converted for training/evaluation.\n",
      "Preprocessing Dataset electronics-train.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1265/1265 [00:02<00:00, 559.98 Dicts/s]\n",
      "ERROR - haystack.modeling.data_handler.processor -  Unable to convert 5 samples to features. Their ids are : 1167-0-0, 595-0-0, 1099-0-0, 572-0-0, 471-0-0\n",
      "INFO - haystack.modeling.data_handler.data_silo -  \n",
      "INFO - haystack.modeling.data_handler.data_silo -  LOADING DEV DATA\n",
      "INFO - haystack.modeling.data_handler.data_silo -  =================\n",
      "INFO - haystack.modeling.data_handler.data_silo -  Loading dev set from: electronics-validation.json\n",
      "INFO - haystack.modeling.data_handler.data_silo -  Got ya 47 parallel workers to convert 252 dictionaries to pytorch datasets (chunksize = 4)...\n",
      "INFO - haystack.modeling.data_handler.data_silo -   0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0  \n",
      "INFO - haystack.modeling.data_handler.data_silo -  /w\\   /|\\  /|\\  /|\\  /w\\   /|\\  /w\\   /|\\  /w\\   /w\\   /w\\   /w\\   /w\\   /|\\  /w\\   /|\\  /|\\  /|\\  /|\\  /|\\  /|\\  /|\\  /w\\   /w\\   /|\\  /|\\  /|\\  /|\\  /|\\  /|\\  /w\\   /w\\   /|\\  /w\\   /w\\   /|\\  /w\\   /w\\   /|\\  /w\\   /w\\   /|\\  /w\\   /w\\   /w\\   /w\\   /w\\ \n",
      "INFO - haystack.modeling.data_handler.data_silo -  / \\   /'\\   /'\\   /'\\   / \\   /'\\   / \\   /'\\   /'\\   /'\\   / \\   /'\\   /'\\   /'\\   / \\   /'\\   /'\\   /'\\   /'\\   /'\\   /'\\   /'\\   / \\   /'\\   /'\\   /'\\   /'\\   /'\\   /'\\   /'\\   /'\\   /'\\   /'\\   /'\\   / \\   /'\\   / \\   / \\   /'\\   /'\\   / \\   /'\\   / \\   / \\   / \\   / \\   /'\\ \n",
      "Preprocessing Dataset electronics-validation.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 252/252 [00:00<00:00, 427.44 Dicts/s]\n",
      "INFO - haystack.modeling.data_handler.data_silo -  \n",
      "INFO - haystack.modeling.data_handler.data_silo -  LOADING TEST DATA\n",
      "INFO - haystack.modeling.data_handler.data_silo -  =================\n",
      "INFO - haystack.modeling.data_handler.data_silo -  No test set is being loaded\n",
      "INFO - haystack.modeling.data_handler.data_silo -  \n",
      "INFO - haystack.modeling.data_handler.data_silo -  DATASETS SUMMARY\n",
      "INFO - haystack.modeling.data_handler.data_silo -  ================\n",
      "INFO - haystack.modeling.data_handler.data_silo -  Examples in train: 2613\n",
      "INFO - haystack.modeling.data_handler.data_silo -  Examples in dev  : 524\n",
      "INFO - haystack.modeling.data_handler.data_silo -  Examples in test : 0\n",
      "INFO - haystack.modeling.data_handler.data_silo -  Total examples   : 3137\n",
      "INFO - haystack.modeling.data_handler.data_silo -  \n",
      "INFO - haystack.modeling.data_handler.data_silo -  Longest sequence length observed after clipping:     384\n",
      "INFO - haystack.modeling.data_handler.data_silo -  Average sequence length after clipping: 290.2606199770379\n",
      "INFO - haystack.modeling.data_handler.data_silo -  Proportion clipped:      0.507079984691925\n",
      "INFO - haystack.modeling.data_handler.data_silo -  [Haystack Tip] 50.7% of your samples got cut down to 384 tokens. Consider increasing max_seq_len. This will lead to higher memory consumption but is likely to improve your model performance\n",
      "INFO - haystack.modeling.model.optimization -  Loading optimizer `AdamW`: '{'correct_bias': False, 'weight_decay': 0.01, 'lr': 1e-05}'\n",
      "INFO - haystack.modeling.model.optimization -  Multi-GPU Training via DataParallel\n",
      "INFO - haystack.modeling.model.optimization -  Using scheduler 'get_linear_schedule_with_warmup'\n",
      "INFO - haystack.modeling.model.optimization -  Loading schedule `get_linear_schedule_with_warmup`: '{'num_training_steps': 164, 'num_warmup_steps': 32}'\n",
      "INFO - haystack.reader.farm -  Saving reader model to ../../saved_models/deepset/minilm-uncased-squad2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training\n"
     ]
    }
   ],
   "source": [
    "# Now its time to train and fine tune the reader\n",
    "#hide_output\n",
    "train_filename = \"electronics-train.json\"\n",
    "dev_filename = \"electronics-validation.json\"\n",
    "\n",
    "reader.train(data_dir = \".\", use_gpu = True, n_epochs = 1, batch_size = 16,\n",
    "             train_filename = train_filename, dev_filename = dev_filename)\n",
    "print(\"Done training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a4ac73-d026-46de-8ea5-863c1f71407e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
