{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8169f0f7-fce5-43fa-84a8-152dd78f8121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT — or Bidirectional Encoder Representations from Transformers — is a hugely popular transformer model used for almost everything in NLP.\n",
    "# Through 12 (or so) encoder layers, BERT encodes a huge amount of information into a set of dense vectors.\n",
    "# Each dense vector typically contains 768 values — and we usually have 512 of these vectors for each sentence encoded by BERT.\n",
    "# These vectors contain what we can view as numerical representations of language. We can also extract those vectors — \n",
    "# from different layers if wanted — but typically from the final layer.\n",
    "# Now, with two correctly encoded dense vectors, we can use a similarity metric like Cosine similarity to calculate their semantic similarity.\n",
    "# Vectors that are more aligned are more semantically alike, and vise-versa.\n",
    "\n",
    "# But there’s one problem, each sequence is represented by 512 vectors — not one vector.\n",
    "# So, this is where another — brilliant — adaption of BERT comes into play. Sentence-BERT allows us to create a single vector that represents our\n",
    "# full sequence, otherwise known as a sentence vector [2].\n",
    "# We have two ways of implementing SBERT — the easy way using the sentence-tranformers library, or the slightly less easy way using transformers and PyTorch.\n",
    "# We’ll cover both, starting with the transformers with PyTorch approach so that we can get an intuition for how these vectors are built.\n",
    "# If you’ve used the HF transformers library, the first few steps will look very familiar. \n",
    "# We initialize our SBERT model and tokenizer, tokenize our text, and process our tokens through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4532fd37-33b4-4046-9c64-b4aec436cf19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 101, 6379, 2003, 1996, 2190, 2103, 1999, 1996, 3224,  102,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets use the not so easy way (transformers and PyTorch)\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "a = \"purple is the best city in the forest\"\n",
    "b = \"there is an art to getting your way and throwing bananas on to the street is not it\"  # this is very similar to 'g'\n",
    "c = \"it is not often you find soggy bananas on the street\"\n",
    "d = \"green should have smelled more tranquil but somehow it just tasted rotten\"\n",
    "e = \"joyce enjoyed eating pancakes with ketchup\"\n",
    "f = \"as the asteroid hurtled toward earth becky was upset her dentist appointment had been canceled\"\n",
    "g = \"to get your way you must not bombard the road with yellow fruit\"  # this is very similar to 'b'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')\n",
    "\n",
    "# Tokenize all of our sentences.\n",
    "tokens = tokenizer([a, b, c, d, e, f, g],\n",
    "                          max_length = 128,\n",
    "                          truncation = True,\n",
    "                          padding = 'max_length',\n",
    "                          return_tensors = 'pt')\n",
    "tokens.keys()\n",
    "tokens['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35a1c2c9-96a6-4f10-9265-37a8f6276652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['last_hidden_state', 'pooler_output'])\n",
      "tensor([[-0.6239, -0.2058,  0.0411,  ...,  0.1490,  0.5681,  0.2381],\n",
      "        [-0.3694, -0.1485,  0.3780,  ...,  0.4204,  0.5553,  0.1441],\n",
      "        [-0.7221, -0.3813,  0.2031,  ...,  0.0761,  0.5162,  0.2813],\n",
      "        ...,\n",
      "        [-0.1894, -0.3711,  0.3034,  ...,  0.1536,  0.3265,  0.1376],\n",
      "        [-0.2496, -0.5227,  0.2341,  ...,  0.3419,  0.3164,  0.0256],\n",
      "        [-0.3311, -0.4430,  0.3492,  ...,  0.3655,  0.2910,  0.0728]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "torch.Size([128, 768])\n"
     ]
    }
   ],
   "source": [
    "# Process our tokenized tensors through the model.\n",
    "outputs = model(**tokens)\n",
    "print(outputs.keys())\n",
    "\n",
    "embeddings = outputs.last_hidden_state\n",
    "print(embeddings[0])\n",
    "print(embeddings[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3db5799c-c433-4ae7-bf62-cd1a1e4bed0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 128, 768])\n",
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[-0.6239, -0.2058,  0.0411,  ...,  0.1490,  0.5681,  0.2381],\n",
      "        [-0.3694, -0.1485,  0.3780,  ...,  0.4204,  0.5553,  0.1441],\n",
      "        [-0.7221, -0.3813,  0.2031,  ...,  0.0761,  0.5162,  0.2813],\n",
      "        ...,\n",
      "        [-0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "torch.Size([7, 768])\n",
      "torch.Size([7, 768])\n",
      "torch.Size([7, 768])\n"
     ]
    }
   ],
   "source": [
    "# We have our vectors of length 768 — but these are not sentence vectors as we have a vector representation for each token in our sequence\n",
    "# (128 here as we are using SBERT — for BERT-base this is 512). We need to perform a mean pooling operation to create the sentence vector.\n",
    "\n",
    "# The first thing we do is multiply each value in our embeddings tensor by its respective attention_mask value.\n",
    "# The attention_mask contains ones where we have ‘real tokens’ (eg not padding tokens), and zeros elsewhere — \n",
    "# this operation allows us to ignore non-real tokens.\n",
    "\n",
    "mask = tokens['attention_mask'].unsqueeze(-1).expand(embeddings.size()).float()\n",
    "print(mask.shape)\n",
    "print(mask[0])\n",
    "\n",
    "# Now we have a masking array that has an equal shape to our output embeddings - we multiply those together to apply the masking operation on our outputs.\n",
    "masked_embeddings = embeddings * mask\n",
    "print(masked_embeddings[0])\n",
    "\n",
    "# Sum the remaining embeddings along axis 1 to get a total value in each of our 768 values.\n",
    "summed = torch.sum(masked_embeddings, 1)\n",
    "print(summed.shape)\n",
    "\n",
    "# Next, we count the number of values that should be given attention in each position of the tensor (+1 for real tokens, +0 for non-real).\n",
    "counted = torch.clamp(mask.sum(1), min=1e-9)\n",
    "print(counted.shape)\n",
    "\n",
    "# Finally, we get our mean-pooled values as the summed embeddings divided by the number of values that should be given attention, counted.\n",
    "mean_pooled = summed / counted\n",
    "print(mean_pooled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3f17443-19ee-423e-8aed-ea5ee4eadf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now these are our sentence vectors, using those we can measure similarity by calculating the cosine similarity between each.\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bed1a60c-d2e7-4c13-9364-16375fd49cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00000024  0.1869276   0.28297707  0.29628253  0.27451017  0.1017627\n",
      "   0.21696275]\n",
      " [ 0.1869276   1.00000024  0.72058785  0.51428944  0.11749659  0.1930695\n",
      "   0.66182357]\n",
      " [ 0.28297707  0.72058785  1.00000012  0.4886443   0.23568958  0.17157143\n",
      "   0.5599308 ]\n",
      " [ 0.29628253  0.51428944  0.4886443   0.99999976  0.26985505  0.37889433\n",
      "   0.52388823]\n",
      " [ 0.27451015  0.1174966   0.23568957  0.26985502  0.99999988  0.23422134\n",
      "  -0.01599768]\n",
      " [ 0.10176268  0.19306949  0.1715714   0.37889433  0.23422134  1.\n",
      "   0.22319689]\n",
      " [ 0.21696277  0.66182357  0.5599308   0.52388823 -0.0159977   0.22319691\n",
      "   0.99999994]]\n"
     ]
    }
   ],
   "source": [
    "# convert to numpy array from torch tensor\n",
    "mean_pooled = mean_pooled.detach().numpy()\n",
    "\n",
    "# calculate similarities (will store in array)\n",
    "scores = np.zeros((mean_pooled.shape[0], mean_pooled.shape[0]))\n",
    "for i in range(mean_pooled.shape[0]):\n",
    "    scores[i, :] = cosine_similarity(\n",
    "        [mean_pooled[i]],\n",
    "        mean_pooled\n",
    "    )[0]\n",
    "\n",
    "print(scores)\n",
    "\n",
    "# Now, think back to the earlier note about sentences b and g having essentially identical meaning whilst not sharing any of the same keywords.\n",
    "# We’d hope SBERT and its superior semantic representations of language to identify these two sentences as similar — and \n",
    "# lo-and-behold the similarity between both is our second-highest score at 0.66 (circled above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e81fd83-791e-40a2-a363-d54d73e937f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.18691427  0.28292829  0.2962288   0.27452093  0.10171202\n",
      "   0.21698235]\n",
      " [ 0.18691427  0.99999994  0.72063208  0.5142017   0.11751355  0.19312078\n",
      "   0.66177475]\n",
      " [ 0.28292829  0.72063208  0.99999988  0.48864788  0.23571709  0.1716553\n",
      "   0.55989563]\n",
      " [ 0.2962288   0.5142017   0.48864788  1.          0.26980412  0.37895399\n",
      "   0.52387094]\n",
      " [ 0.27452087  0.11751357  0.2357171   0.26980412  0.99999976  0.23412059\n",
      "  -0.01596567]\n",
      " [ 0.101712    0.19312075  0.1716553   0.3789539   0.23412059  0.99999976\n",
      "   0.22327027]\n",
      " [ 0.21698233  0.66177469  0.55989563  0.52387094 -0.01596566  0.2232703\n",
      "   0.99999994]]\n"
     ]
    }
   ],
   "source": [
    "# Fortunately there is a much easier way to do all these, Which is just to use sentence-transformers directly\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "# We encode the sentences (producing our mean-pooled sentence embeddings) like so:\n",
    "sentence_embeddings = model.encode([a, b, c, d, e, f, g])\n",
    "\n",
    "# And calculate the cosine similarity just like before.\n",
    "scores = np.zeros((sentence_embeddings.shape[0], sentence_embeddings.shape[0]))\n",
    "for i in range(sentence_embeddings.shape[0]):\n",
    "    scores[i, :] = cosine_similarity(\n",
    "        [sentence_embeddings[i]],\n",
    "        sentence_embeddings\n",
    "    )[0]\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03d43e9-aafa-4aeb-852e-db4041d627c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
