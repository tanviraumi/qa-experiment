{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b97e1c-de70-4696-97b9-b46c1b656f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygrok import Grok\n",
    "import json\n",
    "import random\n",
    "\n",
    "def extract_date_from_string(input_string):\n",
    "    date_pattern = '%{MONTHNUM:month}/%{MONTHDAY:day}/%{YEAR:year}'\n",
    "    grok = Grok(date_pattern)\n",
    "    date_dic = grok.match(input_string)\n",
    "    return date_dic['month'] + \"/\" + date_dic['day'] + \"/\" + date_dic['year']\n",
    "\n",
    "def get_organization(participants, organizer_string):\n",
    "    for p in participants:\n",
    "        if p['organization'] in organizer_string:\n",
    "            return p['organization']\n",
    "    return \"Other\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6e25f8-74e4-42f3-a35e-736c20776eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data in json file\n",
    "input_dir = \"/mnt/sda/highspot/new_data/gong_call_transscript_with_summary_chunked/\"\n",
    "output_dir = \"/mnt/sda/highspot/new_data/gong_call_summary_qa_format/\"\n",
    "\n",
    "dicts = []\n",
    "for i in range(0, 25):\n",
    "    input_file = input_dir + str(i) + \".json\"\n",
    "    input_data = json.load(open(input_file))\n",
    "    for key in input_data:\n",
    "        input_doc = input_data[key]['document']\n",
    "        organization = get_organization(input_doc['meta']['participants'], input_doc['meta']['metadata']['organizer'])\n",
    "        title = input_doc['meta']['metadata']['title'] # string\n",
    "        date = extract_date_from_string(input_doc['meta']['metadata']['date']) # string\n",
    "        doc_id = key\n",
    "        for topic in input_doc['topics']:\n",
    "            label = topic['label']\n",
    "            summary = topic['summary']\n",
    "            call_dict = {\n",
    "                'summary': summary,\n",
    "                'organization': organization,\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'label': label,\n",
    "                'id': doc_id\n",
    "            }\n",
    "            dicts.append(call_dict)\n",
    "    print(f\"Processed file {i}.json\")\n",
    "\n",
    "    \n",
    "output_file_path = output_dir + \"all.json\"\n",
    "with open(output_file_path, 'w') as fp:\n",
    "        json.dump(dicts, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2711f9b6-0ad9-44b5-a397-c9cac1baa63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now load the data back in memory\n",
    "import pandas as pd\n",
    "\n",
    "reviews = pd.read_json(output_dir + \"all.json\")\n",
    "\n",
    "summaries = reviews.summary.values\n",
    "organizations = reviews.organization.values\n",
    "titles = reviews.title.values\n",
    "dts = reviews.date.values\n",
    "labels = reviews.label.values\n",
    "ids = reviews.id.values\n",
    "\n",
    "# Convert dates from numpy to string\n",
    "dates = [pd.to_datetime(str(d)).strftime('%Y-%m-%d') for d in dts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c96dfe-304f-4ec2-9157-3da76154cd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{'content': summary, 'meta': {'meeting_id': id_, 'organization': org, 'title' : title, 'date': date, 'label': label}}\n",
    "            for summary, org, title, date, label, id_\n",
    "            in zip(summaries, organizations, titles, dates, labels, ids)]\n",
    "print(f\"Length of dataset: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe089f3-03d3-464b-88fb-2ae6d37eddbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.choice(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f594c6f-448e-40ea-a15a-cdffc1fa17a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since weâ€™re going to work with the dense DPR retrieval method, we let the preprocessor split\n",
    "# our reviews into chunks of length 100 and an overlap of five words. Although our summaries are not very long\n",
    "# So this won't increase the set size by much\n",
    "from haystack.nodes import PreProcessor\n",
    "\n",
    "processor = PreProcessor(\n",
    "    split_by = 'word', \n",
    "    split_length = 100,\n",
    "    split_respect_sentence_boundary = False,\n",
    "    split_overlap = 5)\n",
    "flattened_docs = processor.process(data)\n",
    "\n",
    "print(f\"Length of dataset: {len(flattened_docs)}\")\n",
    "random.choice(flattened_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e33082-3071-4a7b-8306-14560be8237b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now its time to load the data in elastic. First clean up any previous indexes.\n",
    "from haystack.document_stores import ElasticsearchDocumentStore\n",
    "\n",
    "document_store = ElasticsearchDocumentStore(host = \"localhost\", port = 9200, username = \"elastic\", \n",
    "    password = \"Dkmh=pOI=CSukfWwOoxh\", index = \"document\", scheme = \"https\", verify_certs = True,\n",
    "    ca_certs = \"/home/tanvir/work/qa-experiment/http_ca.crt\")\n",
    "document_store.delete_documents()\n",
    "print(\"Initialized the elastic store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce27b3b-9823-4010-a3a9-6237ce6b56d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now load the data\n",
    "document_store.write_documents(flattened_docs)\n",
    "print(f\"Loaded {document_store.get_document_count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f517f50a-7a9c-4544-ae42-dfe2721b42c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now Initiate the dense passage retriever and update embeddings.\n",
    "from haystack.nodes import DensePassageRetriever\n",
    "retriever = DensePassageRetriever(\n",
    "    document_store = document_store,\n",
    "    query_embedding_model = \"facebook/dpr-question_encoder-single-nq-base\",\n",
    "    passage_embedding_model = \"facebook/dpr-ctx_encoder-single-nq-base\",\n",
    "    max_seq_len_query = 64,\n",
    "    max_seq_len_passage = 128,\n",
    "    batch_size = 16,\n",
    "    use_gpu = True,\n",
    "    embed_title = True,\n",
    "    use_fast_tokenizers = True,\n",
    ")\n",
    "document_store.update_embeddings(retriever)\n",
    "print(\"Done updating embeds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b564160-6010-483d-8180-814bafde7164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now init the reader\n",
    "from haystack.nodes import FARMReader\n",
    "import os\n",
    "\n",
    "# Supress the warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "reader = FARMReader(model_name_or_path = \"deepset/roberta-base-squad2\", use_gpu = True, return_no_answer = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a91478-18de-499b-8cd9-b1992748802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the pipeline\n",
    "from haystack.pipelines import ExtractiveQAPipeline\n",
    "pipeline = ExtractiveQAPipeline(reader, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf74e26-8fc1-48c7-9d8b-75c586c7ff12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the pipeline with organization filter\n",
    "from haystack.utils import print_answers\n",
    "import time\n",
    "filter = {'organization': ['American Technologies, Inc.']}\n",
    "q = \"How is the admin experience?\"\n",
    "start_time = time.process_time()\n",
    "answers = pipeline.run(q, params = {\"Retriever\": {\"top_k\": 30}, \"Reader\": {\"top_k\": 10}, \"filters\": filter})\n",
    "elapsed_time = time.process_time() - start_time\n",
    "print(f\"Time taken with filter: {elapsed_time} seconds\")\n",
    "print_answers(answers, details = \"minimal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0c7959-1bc1-46ff-8127-703784d31d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try the document search pipeline\n",
    "from haystack.pipelines import DocumentSearchPipeline\n",
    "from haystack.utils import print_documents\n",
    "\n",
    "search_pipeline = DocumentSearchPipeline(retriever)\n",
    "search_query = \"salesforce integration\"\n",
    "# filter = {'organization': ['Publicis Sapient'], 'label': ['Decision Timeline']}\n",
    "filter = {'organization': ['American Technologies, Inc.']}\n",
    "\n",
    "start_time = time.process_time()\n",
    "search_result = search_pipeline.run(search_query, params = {\"Retriever\": {\"top_k\": 10}, \"filters\": filter})\n",
    "elapsed_time = time.process_time() - start_time\n",
    "print(f\"Time taken with multi: {elapsed_time} seconds\")\n",
    "print_documents(search_result, max_text_len = 512, print_name = True, print_meta = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9936f3fc-5e37-4d6c-bb3d-4bd43e23f567",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
