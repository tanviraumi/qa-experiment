{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc8ca08-ff57-473f-a2ec-6feb7ca85263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep dive with haystack pipelines\n",
    "from haystack import Pipeline\n",
    "from haystack.utils import launch_es\n",
    "from haystack.document_stores import ElasticsearchDocumentStore\n",
    "from haystack.nodes import BM25Retriever, EmbeddingRetriever, FARMReader\n",
    "from haystack.utils import (\n",
    "    print_answers,\n",
    "    print_documents,\n",
    "    fetch_archive_from_http,\n",
    "    convert_files_to_docs,\n",
    "    clean_wiki_text,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bc9c9c-fcb1-4b4b-ba77-8ae594304719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and prepare data - 517 Wikipedia articles for Game of Thrones\n",
    "doc_dir = \"/mnt/sda/haystack/pipeline_deep_dive_data\"\n",
    "s3_url = \"https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/wiki_gameofthrones_txt11.zip\"\n",
    "fetch_archive_from_http(url=s3_url, output_dir=doc_dir)\n",
    "\n",
    "# convert files to dicts containing documents that can be indexed to our datastore\n",
    "got_docs = convert_files_to_docs(dir_path=doc_dir, clean_func=clean_wiki_text, split_paragraphs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7dddc0-0e87-4c3e-944b-436217047776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DocumentStore and index documents\n",
    "document_store = ElasticsearchDocumentStore(host = \"localhost\", port = 9200, username=\"elastic\", \n",
    "    password=\"cMYVjbUMj_8_664gC6R8\", index = \"document\", scheme = \"https\", verify_certs = True,\n",
    "    ca_certs = \"/home/tanvir/work/qa-experiment/http_ca.crt\")\n",
    "document_store.delete_documents()\n",
    "document_store.write_documents(got_docs)\n",
    "\n",
    "# Initialize Sparse retriever\n",
    "bm25_retriever = BM25Retriever(document_store = document_store)\n",
    "\n",
    "# Initialize dense retriever\n",
    "embedding_retriever = EmbeddingRetriever(\n",
    "    document_store,\n",
    "    model_format = \"sentence_transformers\",\n",
    "    embedding_model = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\",\n",
    ")\n",
    "document_store.update_embeddings(embedding_retriever, update_existing_embeddings=False)\n",
    "\n",
    "# Initialize reader\n",
    "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d335b9-891e-440e-837f-7967cd11152b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haystack features many prebuilt pipelines that cover common tasks. Here we have an ExtractiveQAPipeline\n",
    "from haystack.pipelines import ExtractiveQAPipeline\n",
    "from haystack.pipelines import GenerativeQAPipeline, FAQPipeline\n",
    "from haystack.nodes import RAGenerator\n",
    "\n",
    "# Prebuilt pipeline\n",
    "p_extractive_premade = ExtractiveQAPipeline(reader = reader, retriever = bm25_retriever)\n",
    "res = p_extractive_premade.run(\n",
    "    query = \"Who is the father of Arya Stark?\", params = {\"Retriever\": {\"top_k\": 10}, \"Reader\": {\"top_k\": 5}}\n",
    ")\n",
    "print_answers(res, details = \"minimum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf12cdd-a551-42cb-b0e1-a89db68389fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to just do the retrieval step, you can use a DocumentSearchPipeline\n",
    "from haystack.pipelines import DocumentSearchPipeline\n",
    "p_retrieval = DocumentSearchPipeline(bm25_retriever)\n",
    "res = p_retrieval.run(query = \"Who is the father of Arya Stark?\", params = {\"Retriever\": {\"top_k\": 10}})\n",
    "print_documents(res, max_text_len = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5196b879-396b-4ed1-9fc9-87357ad1008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or if you want to use a Generator instead of a Reader, you can initialize a GenerativeQAPipeline like this\n",
    "from haystack.pipelines import GenerativeQAPipeline, FAQPipeline\n",
    "from haystack.nodes import RAGenerator\n",
    "\n",
    "# We set this to True so that the document store returns document embeddings with each document\n",
    "# This is needed by the Generator\n",
    "document_store.return_embedding = True\n",
    "\n",
    "# Initialize generator\n",
    "rag_generator = RAGenerator()\n",
    "\n",
    "# Generative QA\n",
    "p_generator = GenerativeQAPipeline(generator = rag_generator, retriever = embedding_retriever)\n",
    "res = p_generator.run(query = \"Who is the father of Arya Stark?\", params = {\"Retriever\": {\"top_k\": 10}})\n",
    "print_answers(res, details = \"minimum\")\n",
    "\n",
    "# We are setting this to False so that in later pipelines,\n",
    "# we get a cleaner printout\n",
    "document_store.return_embedding = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f35f733-8763-4e43-a6a9-300a009845dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we are going to rebuild the ExtractiveQAPipelines using the generic Pipeline class.\n",
    "# We do this by adding the building blocks that we initialized as nodes in the graph.\n",
    "\n",
    "# Custom built extractive QA pipeline\n",
    "p_extractive = Pipeline()\n",
    "p_extractive.add_node(component = bm25_retriever, name = \"Retriever\", inputs = [\"Query\"])\n",
    "p_extractive.add_node(component = reader, name = \"Reader\", inputs = [\"Retriever\"])\n",
    "\n",
    "# Now we can run it\n",
    "res = p_extractive.run(query = \"Who is the father of Arya Stark?\", params = {\"Retriever\": {\"top_k\": 10}, \"Reader\": {\"top_k\": 5}})\n",
    "print_answers(res, details = \"minimum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79391365-3e8c-487a-8332-d8c61c92e3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelines offer a very simple way to ensemble together different components. In this example, we are going to combine the power of an EmbeddingRetriever\n",
    "# with the keyword based BM25Retriever. Here we use a JoinDocuments node so that the predictions from each retriever can be merged together.\n",
    "\n",
    "from haystack.nodes import JoinDocuments\n",
    "\n",
    "# Create ensembled pipeline\n",
    "p_ensemble = Pipeline()\n",
    "p_ensemble.add_node(component = bm25_retriever, name = \"ESRetriever\", inputs = [\"Query\"])\n",
    "p_ensemble.add_node(component = embedding_retriever, name = \"EmbeddingRetriever\", inputs = [\"Query\"])\n",
    "p_ensemble.add_node(component = JoinDocuments(join_mode = \"concatenate\"), name = \"JoinResults\", inputs = [\"ESRetriever\", \"EmbeddingRetriever\"])\n",
    "p_ensemble.add_node(component = reader, name = \"Reader\", inputs = [\"JoinResults\"])\n",
    "\n",
    "# Run pipeline\n",
    "res = p_ensemble.run(query = \"Who is the father of Arya Stark?\", params = {\"EmbeddingRetriever\": {\"top_k\": 5}, \"ESRetriever\": {\"top_k\": 5}})\n",
    "print_answers(res, details = \"minimum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2359a08f-1fd4-4ec8-bacd-df015afadb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see how we can write a custome node and add it to the pipeline. One good example is \"Decision Nodes\".\n",
    "# Decision Nodes help you route your data so that only certain branches of your Pipeline are run.\n",
    "# One popular use case for such query classifiers is routing keyword queries to Elasticsearch and questions to EmbeddingRetriever + Reader.\n",
    "# With this approach you keep optimal speed and simplicity for keywords while going deep with transformers when it's most helpful.\n",
    "\n",
    "from haystack import BaseComponent\n",
    "from typing import Optional\n",
    "\n",
    "# Always extend BaseComponent when writing custom nodes.\n",
    "class CustomQueryClassifier(BaseComponent):\n",
    "    outgoing_edges = 2\n",
    "    def run(self, query: str):\n",
    "        if \"?\" in query:\n",
    "            return {}, \"output_2\"\n",
    "        else:\n",
    "            return {}, \"output_1\"\n",
    "\n",
    "\n",
    "# Here we build the pipeline\n",
    "p_classifier = Pipeline()\n",
    "p_classifier.add_node(component = CustomQueryClassifier(), name = \"QueryClassifier\", inputs = [\"Query\"])\n",
    "p_classifier.add_node(component = bm25_retriever, name = \"ESRetriever\", inputs = [\"QueryClassifier.output_1\"])\n",
    "p_classifier.add_node(component = embedding_retriever, name = \"EmbeddingRetriever\", inputs = [\"QueryClassifier.output_2\"])\n",
    "p_classifier.add_node(component = reader, name = \"QAReader\", inputs = [\"ESRetriever\", \"EmbeddingRetriever\"])\n",
    "\n",
    "# Run only the dense retriever on the full sentence query\n",
    "res_1 = p_classifier.run(query=\"Who is the father of Arya Stark?\")\n",
    "print(\"Embedding Retriever Results\" + \"\\n\" + \"=\" * 15)\n",
    "print_answers(res_1)\n",
    "\n",
    "# Run only the sparse retriever on a keyword based query\n",
    "res_2 = p_classifier.run(query=\"Arya Stark father\")\n",
    "print(\"ES Results\" + \"\\n\" + \"=\" * 15)\n",
    "print_answers(res_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd945ef3-7892-4ef7-9740-0d22fc21a3d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
