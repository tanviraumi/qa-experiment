{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b78fb8-32ec-4bfc-866e-a0fd4b9cd0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook we will implement product quantization (PQ) using simple, readable, Python code.\n",
    "x = [1, 8, 3, 9, 1, 2, 9, 4, 5, 4, 6, 2]\n",
    "\n",
    "# The first step is the creation of m subvectors:\n",
    "m = 4\n",
    "D = len(x)\n",
    "\n",
    "# ensure D is divisable by m\n",
    "assert D % m == 0\n",
    "\n",
    "# length of each subvector will be D / m (D* in notation)\n",
    "D_ = int(D / m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5412fb-6a5f-4e88-b844-60b4a843a49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create the subvectors\n",
    "u = [x[row:row+D_] for row in range(0, D, D_)]\n",
    "print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5f03f8-d3f1-40c4-ba51-ec5613b0b0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we must create a set of clusters for each subvector space - giving us m seperate codebooks (codebook will map our subvectors to their assigned cluster centroids - reproduction values).\n",
    "# The clusters would usually be trained, we will not do that here as this example is using only one vector. We will use randomly generated centroid positions.\n",
    "# We need to decide how many centroids create - more centroids == lower error between vector positions and the centroids they are assigned to (more centroids increases the chances of vectors being assigned to a closer centroid).\n",
    "# This value is chosen by k, which must be divisable by m to create equal (sub)centroid ranges for each subvector.\n",
    "\n",
    "k = 2**5\n",
    "assert k % m == 0\n",
    "k_ = int(k/m)\n",
    "print(f\"{k=}, {k_=}\")\n",
    "\n",
    "# We have 32 centroids in total, and 8 centroids per subvector space (subspace).\n",
    "# Each of these centroids will have three dimensions - aligned to our subvector dimensionality. Let's generate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7875c171-cad2-4baf-ab22-aebdb717053e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "c = []  # our overall list of reproduction values\n",
    "for j in range(m):\n",
    "    # each j represents a subvector (and therefore subquantizer) position\n",
    "    c_j = []\n",
    "    for i in range(k_):\n",
    "        # each i represents a cluster/reproduction value position *inside* each subspace j\n",
    "        c_ji = [randint(0, 9) for _ in range(D_)]\n",
    "        c_j.append(c_ji)  # add cluster centroid to subspace list\n",
    "    # add subspace list of centroids to overall list\n",
    "    c.append(c_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97daea3a-baff-467f-8271-64acc7699ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are a lot of centroids in here so the easiest way for us to see them is to visualize:\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "for j in range(m):\n",
    "    ax = fig.add_subplot(2, 2, j + 1, projection = '3d')\n",
    "    # get centroid positions\n",
    "    X = [c[j][i][0] for i in range(k_)]\n",
    "    Y = [c[j][i][1] for i in range(k_)]\n",
    "    Z = [c[j][i][2] for i in range(k_)]\n",
    "    # plot\n",
    "    ax.scatter(X, Y, Z)\n",
    "    ax.set_title(f\"c_{j}\")\n",
    "    # remove tick values (they're messy)\n",
    "    ax.xaxis.set_ticklabels([])\n",
    "    ax.yaxis.set_ticklabels([])\n",
    "    ax.zaxis.set_ticklabels([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1de901f-ec01-49da-9009-753ba72d98bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the centroids for each of our subspaces, subvector u_0 will be mapped to a centroid within subspace c_0, u_1 to c_1, etc, etc.\n",
    "# Let's go ahead and do this. First, we will define a function to find the nearest centroid using Euclidean distance.\n",
    "\n",
    "def euclidean(v, u):\n",
    "    distance = sum((x - y) ** 2 for x, y in zip(v, u)) ** .5\n",
    "    return distance\n",
    "\n",
    "def nearest(c_j, u_j):\n",
    "    distance = 9e9\n",
    "    for i in range(k_):\n",
    "        new_dist = euclidean(c_j[i], u_j)\n",
    "        if new_dist < distance:\n",
    "            nearest_idx = i\n",
    "            distance = new_dist\n",
    "    return nearest_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ef9998-e49b-4191-b7fe-00bcc6b36db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now we calculate the nearest centroids for each subspace.\n",
    "ids = []\n",
    "for j in range(m):\n",
    "    i = nearest(c[j], u[j])\n",
    "    ids.append(i)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b8bcb8-d0e2-459d-ac57-7e80a61c8ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we need a way to translate these IDs back into the centroid co-ordinates - well, we already have it -\n",
    "# our codebook c, when it comes to comparing vectors we don't use the centroid IDs, we use the centroids themselves (our reproduction values).\n",
    "q = []\n",
    "for j in range(m):\n",
    "    c_ji = c[j][ids[j]]\n",
    "    q.extend(c_ji)\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0654634f-8378-41d1-b7b7-8760b9fe33d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We typical measure the error between our quantized vectors q and the originals x using mean squard error (MSE):\n",
    "def mse(v, u):\n",
    "    error = sum((x - y) ** 2 for x, y in zip(v, u)) / len(v)\n",
    "    return error\n",
    "mse(x, q)\n",
    "\n",
    "# When using many vectors, we can to minimize the MSE over our original vectors and the centroids by increasing the number of centroids. However this will increase index size and so must be balanced.\n",
    "# Lower MSE == more accurate search results and higher memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94dd744-927f-4a2c-9d85-70bbb587e9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets try PQ with FAISS\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# now define a function to read the fvecs file format of Sift1M dataset\n",
    "def read_fvecs(fp):\n",
    "    a = np.fromfile(fp, dtype='int32')\n",
    "    d = a[0]\n",
    "    return a.reshape(-1, d + 1)[:, 1:].copy().view('float32')\n",
    "\n",
    "# 1M samples, cut down to 500K\n",
    "xb = read_fvecs('/mnt/sda/vectors/sift/sift_base.fvecs')[:500_000]\n",
    "# queries\n",
    "xq = read_fvecs('/mnt/sda/vectors/sift/sift_query.fvecs')[0].reshape(1, -1)\n",
    "\n",
    "print(xb.shape)\n",
    "print(xq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2ad489-caa8-407d-8685-ccebafdcc414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our first index is a pure PQ implementation using IndexPQ. To initialize the index we need to define three parameters.\n",
    "import faiss\n",
    "\n",
    "\n",
    "# We have our vector dimensionality D, the number of subvectors weâ€™d like to split our full vectors into (we must assert that D is divisible by m).\n",
    "\n",
    "# Finally, we include the nbits parameter. This defines the number of bits that each subquantizer can use, we can translate this into the number\n",
    "# of centroids assigned to each subspace as k_ = 2**nbits. An nbits of 11 leaves us with 2048 centroids per subspace.\n",
    "D = xb.shape[1]\n",
    "m = 8\n",
    "assert D % m == 0\n",
    "nbits = 8  # number of bits per subquantizer, k* = 2**nbits\n",
    "index = faiss.IndexPQ(D, m, nbits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdc0068-e95c-4ea5-9249-77fa5be760be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to train\n",
    "print(index.is_trained)\n",
    "index.train(xb)\n",
    "print(index.is_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9841fda3-6457-43f9-8a53-c55287e19b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.add(xb)  # this is also very slow for large nbits\n",
    "k = 100  # return top k results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7536c7-81a5-4bc2-997a-63d04d453400",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist, I = index.search(xq, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68f317d-2fec-4f4f-afa8-b404e6cdc8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "index.search(xq, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2925af27-37e0-4f63-bc77-6a716e9b386c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search time is nothing special, PQ alone is still an exhaustive search so we would expect nothing spectacular here - but we can make it fast as we'll see later.\n",
    "# Let's compare our results against those produced by a non-quantized flat index.\n",
    "l2_index = faiss.IndexFlatL2(D)\n",
    "l2_index.add(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b13f4f-fde4-4205-8723-88571a861e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "l2_dist, l2_I = l2_index.search(xq, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906b15c2-adb0-4a59-9cd3-4c4283d5c9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([1 for i in I[0] if i in l2_I])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2bb3b5-2a30-48c7-ba3b-5e3844d05106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A recall of 50%, not cutting-edge, but a reasonable sacrifice if this allows us to search larger datasets.\n",
    "# Let's see if PQ has made good on it's promise of reduced memory usage.\n",
    "\n",
    "dir_path = \"/mnt/sda/vectors/perf/\"\n",
    "\n",
    "import os\n",
    "def get_memory(filename, index):\n",
    "    faiss.write_index(index, filename)\n",
    "    file_size = os.path.getsize(filename)\n",
    "    os.remove(filename)\n",
    "    return file_size\n",
    "\n",
    "print(get_memory(dir_path + \"temp.index\", l2_index))\n",
    "print(get_memory(dir_path + \"temp.index\", index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bb9860-959c-417f-b26c-a5dee5dce224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a reduction from 256 MB to 4.1 MB\n",
    "# But what about this not so great search-speed? Is there anything we can do about that? Fortunately yes!\n",
    "# We can improve search-speed by using another quantization step - we add a coarse quantizer, IndexIVF to the process.\n",
    "\n",
    "# But what about this not so great search-speed? Is there anything we can do about that? Fortunately yes!\n",
    "# We can improve search-speed by using another quantization step - we add a coarse quantizer, IndexIVF to the process.\n",
    "\n",
    "vecs = faiss.IndexFlatL2(D)\n",
    "\n",
    "nlist = 2048  # how many Voronoi cells (must be >= k* which is 2**nbits)\n",
    "nbits = 8  # when using IVF+PQ, higher nbits values are not supported\n",
    "index = faiss.IndexIVFPQ(vecs, D, nlist, m, nbits)\n",
    "print(f\"{2**nbits=}\")  # our value for nlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1662bd-eca9-4590-844d-50eaef3db364",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(index.is_trained)\n",
    "index.train(xb)\n",
    "index.add(xb)\n",
    "print(index.is_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4421bca9-e777-4ec7-8707-917154706738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197 Âµs Â± 1.1 Âµs per loop (mean Â± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "dist, I = index.search(xq, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "99247c9f-8103-4968-b9fa-6ddaf69011d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lightning fast search... But how is the recall?\n",
    "sum([1 for i in I[0] if i in l2_I])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3da21549-1650-4faa-b71f-65477a7c9533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "# We can improve the recall by increasing nprobe\n",
    "index.nprobe = 2\n",
    "dist, I = index.search(xq, k)\n",
    "print(sum([1 for i in I[0] if i in l2_I]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7bdb459c-a2f7-4152-ae74-d7a61deb587a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    }
   ],
   "source": [
    "# We can improve the recall by increasing nprobe\n",
    "index.nprobe = 48\n",
    "dist, I = index.search(xq, k)\n",
    "print(sum([1 for i in I[0] if i in l2_I]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2342093f-66c3-4415-bd15-78cb3c7076bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274 Âµs Â± 4.01 Âµs per loop (mean Â± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "dist, I = index.search(xq, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d146f20b-ae02-4f74-b787-5f7604ea0711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.4 ms Â± 736 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "l2_dist, l2_I = l2_index.search(xq, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9b1e5e48-cfe8-4d97-bd9d-fd0ff3f2b11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256000045\n",
      "9196212\n"
     ]
    }
   ],
   "source": [
    "# A significant speed increase from 44ms to 274Âµs, and what are the differences in memory usage?\n",
    "print(get_memory(dir_path + \"temp.index\", l2_index))\n",
    "print(get_memory(dir_path + \"temp.index\", index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac9d872-ac73-4982-9474-fd8dd2adbd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This is a reduction from 256 MB to 9.2 MB. Slightly more than 4.1 MB but still worth it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
